{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fe17437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
    "\n",
    "import mlflow\n",
    "\n",
    "assert \"OPENAI_API_KEY\" in os.environ, \"Please set the OPENAI_API_KEY environment variable.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9936765a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_federal_document(url, div_class):  # noqa: D417\n",
    "    \"\"\"\n",
    "    Scrapes the transcript of the Act Establishing Yellowstone National Park from the given URL.\n",
    "\n",
    "    Args:\n",
    "    url (str): URL of the webpage to scrape.\n",
    "\n",
    "    Returns:\n",
    "    str: The transcript text of the Act.\n",
    "    \"\"\"\n",
    "    # Sending a request to the URL\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        # Parsing the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Finding the transcript section by its HTML structure\n",
    "        transcript_section = soup.find(\"div\", class_=div_class)\n",
    "        if transcript_section:\n",
    "            transcript_text = transcript_section.get_text(separator=\"\\n\", strip=True)\n",
    "            return transcript_text\n",
    "        else:\n",
    "            return \"Transcript section not found.\"\n",
    "    else:\n",
    "        return f\"Failed to retrieve the webpage. Status code: {response.status_code}\"\n",
    "    \n",
    "\n",
    "def fetch_and_save_documents(url_list, doc_path):\n",
    "    \"\"\"\n",
    "    Fetches documents from given URLs and saves them to a specified file path.\n",
    "\n",
    "    Args:# Clean up our temporary directory that we created with our FAISS instance\n",
    "shutil.rmtree(temporary_directory)\n",
    "        url_list (list): List of URLs to fetch documents from.\n",
    "        doc_path (str): Path to the file where documents will be saved.\n",
    "    \"\"\"\n",
    "    for url in url_list:\n",
    "        document = fetch_federal_document(url, \"col-sm-9\")\n",
    "        with open(doc_path, \"a\") as file:\n",
    "            file.write(document)\n",
    "\n",
    "\n",
    "def create_faiss_database(document_path, database_save_directory, chunk_size=500, chunk_overlap=10):\n",
    "    \"\"\"\n",
    "    Creates and saves a FAISS database using documents from the specified file.\n",
    "\n",
    "    Args:\n",
    "\n",
    "retrievalQA = RetrievalQA.from_llm(llm=OpenAI(), retriever=vector_db.as_retriever())\n",
    "\n",
    "\n",
    "# Log the retrievalQA chain\n",
    "def load_retriever(persist_directory):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = FAISS.load_local(\n",
    "        document_path (str): Path to the file containing documents.\n",
    "        database_save_directory (str): Directory where the FAISS database will be saved.\n",
    "        chunk_size (int, optional): Size of each document chunk. Default is 500.\n",
    "        chunk_overlap (int, optional): Overlap between consecutive chunks. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "        FAISS database instance.\n",
    "    \"\"\"\n",
    "    # Load documents from the specified file\n",
    "    document_loader = TextLoader(document_path)\n",
    "    raw_documents = document_loader.load()\n",
    "\n",
    "    # Split documents into smaller chunks with specified size and overlap\n",
    "    document_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    document_chunks = document_splitter.split_documents(raw_documents)\n",
    "\n",
    "    # Generate embeddings for each document chunk\n",
    "    embedding_generator = OpenAIEmbeddings()\n",
    "    faiss_database = FAISS.from_documents(document_chunks, embedding_generator)\n",
    "\n",
    "    # Save the FAISS database to the specified directory\n",
    "    faiss_database.save_local(database_save_directory)\n",
    "\n",
    "    return faiss_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb608d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1701, which is longer than the specified 500\n",
      "Created a chunk of size 865, which is longer than the specified 500\n",
      "Created a chunk of size 1327, which is longer than the specified 500\n",
      "Created a chunk of size 1495, which is longer than the specified 500\n",
      "Created a chunk of size 2024, which is longer than the specified 500\n",
      "Created a chunk of size 971, which is longer than the specified 500\n",
      "Created a chunk of size 680, which is longer than the specified 500\n",
      "Created a chunk of size 855, which is longer than the specified 500\n",
      "Created a chunk of size 737, which is longer than the specified 500\n",
      "Created a chunk of size 1842, which is longer than the specified 500\n",
      "Created a chunk of size 1267, which is longer than the specified 500\n",
      "Created a chunk of size 517, which is longer than the specified 500\n",
      "Created a chunk of size 590, which is longer than the specified 500\n",
      "Created a chunk of size 665, which is longer than the specified 500\n",
      "Created a chunk of size 1169, which is longer than the specified 500\n",
      "Created a chunk of size 549, which is longer than the specified 500\n",
      "Created a chunk of size 658, which is longer than the specified 500\n",
      "Created a chunk of size 544, which is longer than the specified 500\n",
      "Created a chunk of size 607, which is longer than the specified 500\n",
      "Created a chunk of size 545, which is longer than the specified 500\n",
      "Created a chunk of size 545, which is longer than the specified 500\n",
      "Created a chunk of size 658, which is longer than the specified 500\n",
      "Created a chunk of size 1467, which is longer than the specified 500\n",
      "Created a chunk of size 1311, which is longer than the specified 500\n",
      "Created a chunk of size 816, which is longer than the specified 500\n",
      "Created a chunk of size 537, which is longer than the specified 500\n",
      "Created a chunk of size 741, which is longer than the specified 500\n",
      "Created a chunk of size 530, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_text/paper.txt\n"
     ]
    }
   ],
   "source": [
    "temporary_directory = tempfile.mkdtemp()\n",
    "\n",
    "# doc_path = os.path.join(temporary_directory, \"docs.txt\")\n",
    "doc_path = \"local_text/paper.txt\"\n",
    "persist_dir = os.path.join(temporary_directory, \"faiss_index\")\n",
    "\n",
    "print(doc_path)\n",
    "url_listings = [\n",
    "    \"https://www.archives.gov/milestone-documents/act-establishing-yellowstone-national-park#transcript\",\n",
    "    \"https://www.archives.gov/milestone-documents/sherman-anti-trust-act#transcript\",\n",
    "]\n",
    "\n",
    "# fetch_and_save_documents(url_listings, doc_path)\n",
    "\n",
    "vector_db = create_faiss_database(doc_path, persist_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dd3caea",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"Legal RAG\")\n",
    "\n",
    "retrievalQA = RetrievalQA.from_llm(llm=OpenAI(), retriever=vector_db.as_retriever())\n",
    "\n",
    "\n",
    "# Log the retrievalQA chain\n",
    "def load_retriever(persist_directory):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = FAISS.load_local(\n",
    "        persist_directory,\n",
    "        embeddings,\n",
    "        allow_dangerous_deserialization=True,  # This is required to load the index from MLflow\n",
    "    )\n",
    "    return vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    model_info = mlflow.langchain.log_model(\n",
    "        retrievalQA,\n",
    "        name=\"retrieval_qa\",\n",
    "        loader_fn=load_retriever,\n",
    "        persist_dir=persist_dir,\n",
    "    )\n",
    "\n",
    "def print_formatted_response(response_list, max_line_length=80):\n",
    "    \"\"\"\n",
    "    Formats and prints responses with a maximum line length for better readability.\n",
    "\n",
    "    Args:\n",
    "    response_list (list): A list of strings representing responses.\n",
    "    max_line_length (int): Maximum number of characters in a line. Defaults to 80.\n",
    "    \"\"\"\n",
    "    for response in response_list:\n",
    "        words = response.split()\n",
    "        line = \"\"\n",
    "        for word in words:\n",
    "            if len(line) + len(word) + 1 <= max_line_length:\n",
    "                line += word + \" \"\n",
    "            else:\n",
    "                print(line)\n",
    "                line = word + \" \"\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a137cf48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/15 11:24:48 INFO mlflow.tracking.fluent: Active model is set to the logged model with ID: m-503495f85042481c825464093cd20245\n",
      "2025/07/15 11:24:48 INFO mlflow.tracking.fluent: Use `mlflow.set_active_model` to set the active model to a different one if needed.\n"
     ]
    }
   ],
   "source": [
    "loaded_model = mlflow.pyfunc.load_model(model_info.model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eecc2546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I do not know. \n"
     ]
    }
   ],
   "source": [
    "answer1 = loaded_model.predict([{\"query\": \"What does the document say?\"}])\n",
    "\n",
    "print_formatted_response(answer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e87d777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCPATr is a transformer architecture that incorporates both graph convolution \n",
      "and position awareness. It uses a multi-head self-attention mechanism and \n",
      "incorporates graph convolution operations in critical places in its \n",
      "architecture. Its overall processing pipeline and the position aware \n",
      "self-attention block are depicted in Figure \\ref{Overall_GCPATr_Pipeline}. \n"
     ]
    }
   ],
   "source": [
    "answer1 = loaded_model.predict([{\"query\": \"Explain GCPATR\"}])\n",
    "\n",
    "print_formatted_response(answer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1acf677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward function is defined as the decrease in the moving average of mean \n",
      "square error, penalized by the communication costs incurred during the round. \n"
     ]
    }
   ],
   "source": [
    "answer1 = loaded_model.predict([{\"query\": \"What is the reward function?\"}])\n",
    "\n",
    "print_formatted_response(answer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75ac7e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The communication cost affects the results by penalizing the amount of \n",
      "communication resources used in achieving the model training. This can lead to \n",
      "improved system efficiency by lowering the validation error and using minimal \n",
      "communication resources. \n"
     ]
    }
   ],
   "source": [
    "answer1 = loaded_model.predict([{\"query\": \"How does communication cost affect the results?\"}])\n",
    "\n",
    "print_formatted_response(answer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6fb83f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The use of reinforcement learning in the proposed framework has shown to \n",
      "improve the final RUL prediction error and reduce communication costs, as \n",
      "evidenced by the results presented in the tables and figures. It also enables \n",
      "better exploration and convergence speed, as seen in the smoother convergence \n",
      "plots. However, further research is needed to optimize the value function and \n",
      "explore the performance of different reinforcement learning algorithms in noisy \n",
      "communication channels. \n"
     ]
    }
   ],
   "source": [
    "answer1 = loaded_model.predict([{\"query\": \"How does reinforcement learning affect the results?\"}])\n",
    "\n",
    "print_formatted_response(answer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe0c64dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "{context}\n",
      "\n",
      "Question: {question}\n",
      "Helpful Answer:\n"
     ]
    }
   ],
   "source": [
    "print(retrievalQA.combine_documents_chain.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a476bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "{context}\n",
      "\n",
      "Question: {question}\n",
      "Helpful Answer:\n"
     ]
    }
   ],
   "source": [
    "print(retrievalQA.combine_documents_chain.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d188ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up our temporary directory that we created with our FAISS instance\n",
    "shutil.rmtree(temporary_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61072ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/tmpoywmkr6k'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temporary_directory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
