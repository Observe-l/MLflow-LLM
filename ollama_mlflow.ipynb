{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bf050cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
    "from langchain_ollama import OllamaLLM\n",
    "from termcolor import colored\n",
    "import re\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3150c769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_faiss_database(document_path, database_save_directory, chunk_size=500, chunk_overlap=10):\n",
    "    \"\"\"\n",
    "    Creates and saves a FAISS database using documents from the specified file.\n",
    "\n",
    "    Args:\n",
    "\n",
    "retrievalQA = RetrievalQA.from_llm(llm=OpenAI(), retriever=vector_db.as_retriever())\n",
    "\n",
    "\n",
    "# Log the retrievalQA chain\n",
    "def load_retriever(persist_directory):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = FAISS.load_local(\n",
    "        document_path (str): Path to the file containing documents.\n",
    "        database_save_directory (str): Directory where the FAISS database will be saved.\n",
    "        chunk_size (int, optional): Size of each document chunk. Default is 500.\n",
    "        chunk_overlap (int, optional): Overlap between consecutive chunks. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "        FAISS database instance.\n",
    "    \"\"\"\n",
    "    # Load documents from the specified file\n",
    "    document_loader = TextLoader(document_path)\n",
    "    raw_documents = document_loader.load()\n",
    "\n",
    "    # Split documents into smaller chunks with specified size and overlap\n",
    "    document_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    document_chunks = document_splitter.split_documents(raw_documents)\n",
    "\n",
    "    # Generate embeddings for each document chunk\n",
    "    embedding_generator = OpenAIEmbeddings()\n",
    "    faiss_database = FAISS.from_documents(document_chunks, embedding_generator)\n",
    "\n",
    "    # Save the FAISS database to the specified directory\n",
    "    faiss_database.save_local(database_save_directory)\n",
    "\n",
    "    return faiss_database\n",
    "\n",
    "def print_answer_formatted(answer, max_line_length=100):\n",
    "    \"\"\"\n",
    "    Prints the answer with the following requirements:\n",
    "    1. Max length of each line is 160.\n",
    "    2. <think> ... </think> content is printed in a light color.\n",
    "    3. After <think> content, print 2 empty lines.\n",
    "    \"\"\"\n",
    "    # Extract <think> ... </think> content\n",
    "    think_match = re.search(r\"<think>(.*?)</think>\", answer, re.DOTALL)\n",
    "    if think_match:\n",
    "        think_content = think_match.group(1).strip()\n",
    "        rest_content = answer.replace(think_match.group(0), \"\").strip()\n",
    "    else:\n",
    "        think_content = \"\"\n",
    "        rest_content = answer\n",
    "\n",
    "    # Helper to print with max line length\n",
    "    def print_wrapped(text, color=None):\n",
    "        words = text.split()\n",
    "        line = \"\"\n",
    "        for word in words:\n",
    "            if len(line) + len(word) + 1 <= max_line_length:\n",
    "                line += word + \" \"\n",
    "            else:\n",
    "                if color:\n",
    "                    print(colored(line.rstrip(), color))\n",
    "                else:\n",
    "                    print(line.rstrip())\n",
    "                line = word + \" \"\n",
    "        if line:\n",
    "            if color:\n",
    "                print(colored(line.rstrip(), color))\n",
    "            else:\n",
    "                print(line.rstrip())\n",
    "\n",
    "    # Print <think> content in light color (e.g., 'cyan')\n",
    "    if think_content:\n",
    "        print_wrapped(think_content, color=\"cyan\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # Print the rest\n",
    "    if rest_content:\n",
    "        print_wrapped(rest_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d840032",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1701, which is longer than the specified 500\n",
      "Created a chunk of size 865, which is longer than the specified 500\n",
      "Created a chunk of size 1327, which is longer than the specified 500\n",
      "Created a chunk of size 1495, which is longer than the specified 500\n",
      "Created a chunk of size 2024, which is longer than the specified 500\n",
      "Created a chunk of size 971, which is longer than the specified 500\n",
      "Created a chunk of size 680, which is longer than the specified 500\n",
      "Created a chunk of size 855, which is longer than the specified 500\n",
      "Created a chunk of size 737, which is longer than the specified 500\n",
      "Created a chunk of size 1842, which is longer than the specified 500\n",
      "Created a chunk of size 1267, which is longer than the specified 500\n",
      "Created a chunk of size 517, which is longer than the specified 500\n",
      "Created a chunk of size 590, which is longer than the specified 500\n",
      "Created a chunk of size 665, which is longer than the specified 500\n",
      "Created a chunk of size 1169, which is longer than the specified 500\n",
      "Created a chunk of size 549, which is longer than the specified 500\n",
      "Created a chunk of size 658, which is longer than the specified 500\n",
      "Created a chunk of size 544, which is longer than the specified 500\n",
      "Created a chunk of size 607, which is longer than the specified 500\n",
      "Created a chunk of size 545, which is longer than the specified 500\n",
      "Created a chunk of size 545, which is longer than the specified 500\n",
      "Created a chunk of size 658, which is longer than the specified 500\n",
      "Created a chunk of size 1467, which is longer than the specified 500\n",
      "Created a chunk of size 1311, which is longer than the specified 500\n",
      "Created a chunk of size 816, which is longer than the specified 500\n",
      "Created a chunk of size 537, which is longer than the specified 500\n",
      "Created a chunk of size 741, which is longer than the specified 500\n",
      "Created a chunk of size 530, which is longer than the specified 500\n"
     ]
    }
   ],
   "source": [
    "temporary_directory = tempfile.mkdtemp()\n",
    "\n",
    "# doc_path = os.path.join(temporary_directory, \"docs.txt\")\n",
    "doc_path = \"local_text/paper.txt\"\n",
    "persist_dir = os.path.join(temporary_directory, \"faiss_index\")\n",
    "\n",
    "# fetch_and_save_documents(url_listings, doc_path)\n",
    "\n",
    "vector_db = create_faiss_database(doc_path, persist_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac4db14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lwh/.conda/envs/ollama/lib/python3.10/site-packages/mlflow/pyfunc/utils/data_validation.py:186: UserWarning: \u001b[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001b[0m\n",
      "  color_warning(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7058dc7b55b547c684c181de36331717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/07/18 18:02:30 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(\"Ollama RAG\")\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "\n",
    "code_path = \"ollama_pyfunction.py\"\n",
    "with mlflow.start_run() as run:\n",
    "    model_info = mlflow.pyfunc.log_model(\n",
    "        name=\"gemma3-12b\",\n",
    "        python_model=code_path,\n",
    "        artifacts={\n",
    "            \"persist_directory\": persist_dir,\n",
    "        },\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7874ef93",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info.model_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a1f043e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lwh/.conda/envs/ollama/lib/python3.10/site-packages/mlflow/pyfunc/utils/data_validation.py:186: UserWarning: \u001b[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001b[0m\n",
      "  color_warning(\n"
     ]
    }
   ],
   "source": [
    "loaded_model = mlflow.pyfunc.load_model(model_info.model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d952e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCPATr is a Graph Convolution Position Aware Transformer architecture that enhances the\n",
      "non-Euclidean interdependency modeling power of PATr by incorporating graph convolution operations.\n"
     ]
    }
   ],
   "source": [
    "query1 = {\"query\": \"What is GCPATr?\"}\n",
    "answer1 = loaded_model.predict(query1)\n",
    "print_answer_formatted (answer1['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df41c25",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PyFuncModel' object has no attribute 'vectorstore'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloaded_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorstore\u001b[49m\u001b[38;5;241m.\u001b[39mas_retriever()\u001b[38;5;241m.\u001b[39mget_relevant_documents(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is GCPATr?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PyFuncModel' object has no attribute 'vectorstore'"
     ]
    }
   ],
   "source": [
    "vector_db.as_retriever().get_relevant_documents(\"What is GCPATr?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0126ed99",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer2 = loaded_model.predict({\"query\": \"Can you summerize this paper?\"})\n",
    "print_answer_formatted (answer2['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6ac021",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer3 = loaded_model.predict({\"query\": \"Can you repeat my previous question?\"})\n",
    "print_answer_formatted (answer3['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31ec541",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = mlflow.pyfunc.load_model(model_info.model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f7cb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info.model_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18272f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = mlflow.pyfunc.load_model(model_info.model_uri)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
